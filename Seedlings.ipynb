{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seedlings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYX6sSIqrsSEH7lQ8r/v6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Czarczynski/API-ekspolracja-danych-projekt/blob/main/Seedlings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFPTddCwL7gK"
      },
      "source": [
        "Projekt dotyczył klasyfikowania sadzonek roślin na podstawie ich zdjęć. Dane były zapisane w dwóch folderach - train oraz test. Struktura folderu train składała się z dwunastu podfolderów nazwanych gatunkami roślin, które się w nich znajdowały. Folder test zawierał same zdjęcia bez etykiet, służył on do stworzenia submisji na portalu kaggle. W tym projekcie postanowiliśmy nauczyć naszą sieć z folderu train i nie wysyłać wyników na kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVm3x3-uL-dO"
      },
      "source": [
        "**Podsumowanie głównych bibliotek**\n",
        "\n",
        "*   `os` służy jako interfejs do obsługi systemów operacyjnych, my go użyjemy do listowania folderów i plików;\n",
        "*   `numpy` przyda się nam do konwersji danych;\n",
        "*  `pandas` użyjemy do przeprowadzenia krótkiej analizy eksploracyjnej;\n",
        "*   `cv2` służy do transformowania zdjęć;\n",
        "*   `keras`, `sklearn` oraz `tensorflow` służą do obsługi modeli sieci neuronowych;\n",
        "*   `matplotlib.pyplot` użyjemy do wyświetlenia zdjęć oraz wykresów zależności straty i dokładności od epochów;\n",
        "*   `tqdm` służy do wyświetlenia progresu iteracji w pętli;\n",
        "*   `google.colab.patches` naprawia błąd z wyświetlaniem zdjęć z biblioteki cv2.\n",
        "\n",
        "Do prawidłowego działania tego programu należy pobrać dane wejściowe na dysk Google, podać do programu ścieżkę oraz podłączyć dysk do notatnika w Colabie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFcf51kNMEbJ"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalMaxPooling2D\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZlVB_6kN25t"
      },
      "source": [
        "Poniższy chunk kodu służy do załadowania zdjęć do tablic. Zmienna path prowadzi nas do folderu, w których znajdują się podfoldery ze zdjęciami. Zmienna foldery tworzy listę wszystkich podfolderów, po których iterujemy w pętli, aby zapisać wszystkie zdjęcia do tablic. Zdjęcie jest trochę zmniejszone, aby zredukować czas ładowania oraz pamięć potrzebną do przechowania ich. Zapisujemy również etykietę danego zdjęcia biorąc nazwę folderu, w którym obecnie się znajdujemy. file_path służy jako kontrola, czy wszystko dobrze się zapisuje.\n",
        "Zakomentowana komenda tworzy okrojony zbiór danych, w którym we wszystkich klasach jest tyle samo zdjęć. Będziemy również rozpatrywać model wytrenowany właśnie na danych uszczuplonych."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTGZwCXqOCg5"
      },
      "source": [
        "path = \"/content/drive/My Drive/Projekt_SN/train\"\n",
        "pwd = os.getcwd()\n",
        "foldery = os.listdir(path) \n",
        "train_labels = []\n",
        "train_images = []\n",
        "file_path = []\n",
        "i=1\n",
        "\n",
        "for folder in tqdm(foldery):\n",
        "    fol_path = path + '/'+ folder\n",
        "    files = os.listdir(fol_path)\n",
        "    for file in files:\n",
        "    #for file in files[:220]:\n",
        "        img_path = fol_path + '/' + file\n",
        "        train_labels.append(folder)\n",
        "        train_images.append(cv2.resize(cv2.imread(img_path), (64, 64)))\n",
        "        file_path.append(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gCRS2t0ODuy"
      },
      "source": [
        "len(train_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1fluFayOGGH"
      },
      "source": [
        "Dane zawierają 2640 zdjęć w przypadku zbioru okrojonego."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2uuRG0XOGpC"
      },
      "source": [
        "print(np.unique(train_labels))\n",
        "print(len(np.unique(train_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDynHoFtOI4w"
      },
      "source": [
        "Mamy 12 gatunków sadzonek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QutVSZGLOKlO"
      },
      "source": [
        "df= pd.DataFrame(train_labels)\n",
        "df.groupby([0]).size().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRDTOyFhOOGT"
      },
      "source": [
        "Jak widać, dane są niezbalansowane. Dlatego też zdecydowaliśmy się na sprawdzenie, czy dane zrównoważone przynoszą lepsze efekty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJBPhOuzOPLd"
      },
      "source": [
        "Poniżej zdefiniowana jest funkcja nakładająca maskę na zdjęcie. Zmienne `lower_green` i `upper_green` definiują granicę koloru zielonego w notacji HSV. \n",
        "\n",
        "Kolory w notacji HSV mają następujące parametry:\n",
        "* H - hue, czyli odcień światła;\n",
        "* S - saturation, czyli nasycenie koloru;\n",
        "* V - value lub inaczej brightness, czyli moc światła białego.\n",
        "\n",
        "Notacja HSV jest nam potrzebna ponieważ funkcja `cv2.inRange` wymaga na wejściu zdjęcia oraz granic właśnie w tym formacie. Jednocześnie, nie tracimy żadnych informacji o zdjęciu, więc możemy spokojnie wykonać taką transformację.\n",
        "\n",
        "`image_hsv` konwertuje zdjęcie z RGB do HSV. `mask` definiuje maskę eliminując wszystkie kolory nie mieszczące się w granicy `lower` - `upper`. `img_mask` nakłada maskę na zdjęcie, a `img_gray` konwertuje zdjęcie z kolorowego do szarego, dzięki czemu wyeliminujemy dwa kanały kolorów i będziemy mieli przyjemniejsze wymiary danych.\n",
        "\n",
        "Transformując zdjęcie do szarości po nałożeniu na nie maski nie tracimy żadnych informacji, gdyż na tym zdjęciu znajdują się tylko elementy o kolorze zielonym.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz3rooHyOQ1R"
      },
      "source": [
        "def img_preprocessing(img):\n",
        "  lower_green = np.array([25, 50, 50])\n",
        "  upper_green = np.array([95, 255, 255])\n",
        "\n",
        "  image_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "  mask = cv2.inRange(image_hsv, lower_green, upper_green)\n",
        "  img_mask = cv2.bitwise_or(img, img, mask = mask)\n",
        "  img_gray = cv2.cvtColor(img_mask, cv2.COLOR_BGR2GRAY)\n",
        "  return img_gray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXi4mcMJOS3B"
      },
      "source": [
        "Poniższy chunk demonstruje działanie funkcji `img_preprocessing`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn3sGcHKOVzg"
      },
      "source": [
        "ind = 4500\n",
        "plt.imshow(train_images[ind])\n",
        "plt.show()\n",
        "print(file_path[ind])\n",
        "img = img_preprocessing(train_images[ind])\n",
        "plt.imshow(img, cmap=plt.get_cmap(\"gray\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4vHgImpOY7j"
      },
      "source": [
        "Tutaj przeprowadzamy ostateczne przekształcenie naszych danych do formy, którą zadamy naszemu modelowi. Tworzymy tabelę trainX, która będzie zawierała informację o zdjęciach i trainY, która zawiera etykietę danego zdjęcia. Następnie przekształcamy tabelę z etykietami w wektor i aplikujemy One-Hot encoding. Transformacja ta polega na przedstawieniu klas jako wektory zero-jedynkowe, z jedynką w dokładnie jednym miejscu - odpowiadającym danej klasie. Następnie następuje przeskalowanie informacji o zdjęciach do przedziału $[0;1]$ oraz przetasowanie wartości. Przetasowanie jest potrzebne ponieważ przed nim wartości ustawione są 'po kolei', czyli najpierw są zdjęcia z pierwszej klasy, dalej z drugiej itd. Zbiór walidacyjny w modelu będziemy tworzyć poprzez parametr `validation_split`, więc przy uporządkowanym zbiorze danych walidacja odbywałaby się w klasach niewidzianych wcześniej przez model. Ostatnia linijka to dodanie wymiaru, konkretnie kanału koloru - w naszym przypadku mamy tylko jeden kanał, czyli szarość."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFcVyGcqOZpy"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "trainX = []\n",
        "trainY = []\n",
        "for i in range(len(train_images)):\n",
        "  trainX.append(img_preprocessing(train_images[i]))\n",
        "  trainY.append(train_labels[i])\n",
        "trainY = np.asarray(trainY)\n",
        "trainY = trainY.reshape(-1,1)\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
        "trainY = ohe.fit_transform(trainY)\n",
        "trainX = np.asarray(trainX)\n",
        "trainX = trainX/255\n",
        "trainX, trainY = shuffle(trainX, trainY)\n",
        "trainX = tf.expand_dims(trainX, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}